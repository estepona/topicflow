{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to TopicFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TopicFlow is a tool that visualizes the results of automatic topic detection and topic alignment between sets of tweets over time. The tool was developed by Jianyu Li, Sana Malik, Panagis (Pano) Papadatos and Alison Smith originally as a team project for CMSC 734 Information Visualization at the University of Maryland. You can find more information about TopicFlow by reading the README.md and their papers:\n",
    "- [TopicFlow: Visualizing Topic Alignment of Twitter Data over Time](https://wiki.cs.umd.edu/cmsc734_f12/images/0/05/TopicFlowFinalReport2.pdf)\n",
    "- [Visual Analysis of Topical Evolution in Unstructured Text: Design and Evaluation of TopicFlow](http://link.springer.com/chapter/10.1007/978-3-319-19003-7_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to achieve by utilizing TopicFlow for [PERCEIVE](https://github.com/sailuh/perceive) is trying to visualize the \"flow\" of topics of Full Disclosure documents that may help us identify upcoming cybersecurity threats. \n",
    "\n",
    "PERCEIVE is developed and maintained by a joint effort of many contributors. The role of TopicFlow in PERCEIVE can be simplified with the graph below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![work flow](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/work%20flow%20diagram/work%20flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output of TopicFlow pipeline is the visualization, the output of this data transformation pipeline is a *run.py* file that enables a user to create new TopiFlow projects or run an existing project. \n",
    "\n",
    "Although TopicFlow is a powerful tool, it was designed to visualize only the flow of \"tweets\". To make TopicFlow work for Full Disclosure data, several changes were made to the original scripts:\n",
    "1. changed all \"tweet\" related content to \"doc\" or \"document\" in the final visualization;\n",
    "2. disabled \n",
    "```javascript\n",
    "if ($(\"g #\"+j)[0].style.display != \"none\") { }\n",
    "``` \n",
    "in *controller.js* to avoid `style errors`. Otherwise, TopicFlow couldn't configure text data other than tweets.\n",
    "3. removed all datasets to select from except Full Disclosure 2012 dataset. Several changes were made in *index.html*, *controller.js*, and */topicflow/data* directory. For example, in *controller.js*, the original version allows users to choose some of these datasets:\n",
    "```javascript\n",
    "var idToName = {\"HCI\" : \"HCI\", \"ModernFamily\" : \"Modern Family\", \"catfood\": \"Catfood\" , \n",
    "\t\t\t\t\t\"drugs\" : \"Drugs\", \"earthquake\" : \"Earthquake\", \"umd\" : \"UMD\", \"debate\":\"#debate\", \"chi\":\"CHI Conference\", \n",
    "\t\t\t\t\t\"sandy\" : \"Sandy and NJ\"}\n",
    "```\n",
    "however, in our final version, we only need the following dataset as the starting point:\n",
    "```javascript\n",
    "var idToName = {\n",
    "                // add new idToName\n",
    "                \"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So How do we approach this? After exploring TopicFlow and discussing with Carlos Paradis several times, I believe the best way to utilize TopicFlow without overhauling the original codes is modifying only the parts that help us display our datasets. Since TopicFlow is very hand-coded, this data transformation pipeline has to edit the actual scripts and generate new files with our Python program. \n",
    "\n",
    "To better understand what we need to do. Here let's take a look at how TopicFlow works in the simpliest form, a triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![methodology](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/work%20flow%20diagram/methodology.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially two scripts and one data directory controls how TopicFlow works: *index.html* provides the place for the visualization and the basic information, */data/< project >* stores the actual data to display, and *controller.js* coordinates all the JavaScript scripts that tells TopicFlow how to read data and the way to visualize. The highlighted elements are what we will be modifying or creating in this data transformation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that this transformation pipeline only works for Full Disclosure data**  \n",
    "The functions in this pipeline only works for Full Disclosure datasets. To create a new project, the two specified directories after \"-a\" must contain the following files or sub-directories:\n",
    "\n",
    "\n",
    "*path_doc*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; |- yyyy_mm_index.txt  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; |- Full_Disclosure_Mailing_List_mmyyyy.csv   \n",
    "\n",
    "*path_LDA*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; |- Document_Topic_Matrix  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; |- Topic_Flow  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; |- Topic_Term_Matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking Through All Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'll try to explain how each data transformation functon works in a language that's easy to comprehend. The flow of `run.py` looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run.py flow](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/work%20flow%20diagram/run.py%20flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argparse and local server will be covered in Function 6. Another function not mentioned in this notebook is **read_data**, which just reads data and store as pandas.DataFrame objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: A backup of topicflow is included. Use it to restore missing *index.html* and *controller.js* if something went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1 - modify_html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the *index.html* file in TopicFlow controls the loading of datasets and display the dataset selector when user initiates TopicFlow or changes datasets. Respectively, the two parts in index.html looks like:\n",
    "```html\n",
    "<script src=\"data/Full_Disclosure_2012/Tweet.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/Bins.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/TopicSimilarity.js\"></script>\n",
    "\n",
    "<!-- add new section after this line -->\n",
    "<!-- end of adding new datasets. -->\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```html\n",
    "<li id=\"Full_Disclosure_2012\"><a href=\"#\">Full_Disclosure_2012</a></li>\n",
    "<!-- add new dataset selector after this line -->\n",
    "<!-- end of adding new dataset selector -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let the function find the locations of the above parts and add codes for a new dataset in the same style. To make it faster finding the locations, four lines of comments are placed so that the program easily finds the place for our insertion. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modify_html](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/function%20graph/modify_html.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_html(project_name, path_tf):\n",
    "    \"\"\"\n",
    "    Modify the content of \\topicflow\\index.html.\n",
    "    Two hand-added comments are used to locate the lines where new content can be\n",
    "    added. Executing the function would replace the existing index.html.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_tf      -- path of topicflow directory\n",
    "    \"\"\"\n",
    "    # read exisitng index.html and parse by lines\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'r') as file:\n",
    "        html = file.read()\n",
    "\n",
    "    html_parse = html.split('\\n')\n",
    "\n",
    "    # add new section after '<!-- add new section after this line -->'\n",
    "    ix = html_parse.index('<!-- add new section after this line -->')\n",
    "    new_section = '<script src=\"data/SHA/Doc.js\"></script>\\n<script src=\"data/SHA/Bins.js\"></script>\\n<script src=\"data/SHA/TopicSimilarity.js\"></script>\\n'.replace('SHA',project_name)\n",
    "    html_parse.insert(ix+1, new_section)\n",
    "\n",
    "    # add new selector after '<!-- add new dataset selector after this line -->'\n",
    "    ix = html_parse.index('\\t\\t\\t<!-- add new dataset selector after this line -->')\n",
    "    new_selector = '\\t\\t\\t<li id=\"SHA\"><a href=\"#\">SHA</a></li>'.replace('SHA', project_name)\n",
    "    html_parse.insert(ix+1, new_selector)\n",
    "\n",
    "    # replace existing index.html\n",
    "    html_combine = '\\n'.join(html_parse)\n",
    "    os.remove(os.path.join(path_tf, 'index.html'))\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'w') as file:\n",
    "        file.write(html_combine)\n",
    "\n",
    "    print('\\nindex.html modified,        20% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"index.html modified,        20% complete.\" will be printed out in the terminal. The new index.html should have the following changes being made. In this example, the new project is called \"Fre\":\n",
    "```html\n",
    "<script src=\"data/Full_Disclosure_2012/Tweet.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/Bins.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/TopicSimilarity.js\"></script>\n",
    "\n",
    "<!-- add new section after this line -->\n",
    "<script src=\"data/Fre/Doc.js\"></script>\n",
    "<script src=\"data/Fre/Bins.js\"></script>\n",
    "<script src=\"data/Fre/TopicSimilarity.js\"></script>\n",
    "<!-- end of adding new datasets. -->\n",
    "```\n",
    "and\n",
    "```html\n",
    "<li id=\"Full_Disclosure_2012\"><a href=\"#\">Full_Disclosure_2012</a></li>\n",
    "<!-- add new dataset selector after this line -->\n",
    "<li id=\"Fre\"><a href=\"#\">Fre</a></li>\n",
    "<!-- end of adding new dataset selector -->\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2 - modify_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same methodology as **modify_html**, **modify_controller** locates two parts in *controller.js* that controls how TopicFlow reads the data of our new project and what functions to call to parse the data. Respectively, the two parts nested in the function **populateVisualization** in *controller.js* look like:\n",
    "```javascript\n",
    "var idToName = {\n",
    "                // add new idToName\n",
    "                \"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```\n",
    "and\n",
    "```javascript\n",
    "// Populate the interface with the selected data set\n",
    "if (selected_data===\"Full_Disclosure_2012\") {\n",
    "    populate_tweets_Full_Disclosure_2012();\n",
    "    populate_bins_Full_Disclosure_2012();\n",
    "    populate_similarity_Full_Disclosure_2012();\n",
    "}\n",
    "// add new selected dataset here\n",
    "// end of adding new selected datasets\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let the function find the locations of the above parts and add codes for a new idToName variable and a new selected dataset in the same style. To make it faster finding the locations, three lines of comments are placed so that the program easily finds the place for our insertion. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modify_controller](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/function%20graph/modify_controller.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_controller(project_name, path_tf):\n",
    "    \"\"\"\n",
    "    Modify the content of \\topicflow\\scripts\\controller.js.\n",
    "\n",
    "    Two hand-added comments are used to locate the lines where new content can be\n",
    "    added. Executing the function would replace the existing controller.js.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_tf      -- path of topicflow directory\n",
    "    \"\"\"\n",
    "    # read exisitng controller.js and parse by lines\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'r') as file:\n",
    "        controller = file.read()\n",
    "\n",
    "    controller_parse = controller.split('\\n')\n",
    "\n",
    "    # add idToName after '// add new idToName'\n",
    "    ix = controller_parse.index('\\t\\t\\t\\t\\t// add new idToName')\n",
    "    new_idToName = '\\t\\t\\t\\t\\t\"SHA\":\"SHA\",'.replace('SHA', project_name)\n",
    "    controller_parse.insert(ix+1, new_idToName)\n",
    "\n",
    "    # add selected dataset after '// add new selected dataset here'\n",
    "    ix = controller_parse.index('\\t// add new selected dataset here')\n",
    "    new_selectedDataset = '\\tif (selected_data===\"SHA\") {\\n\\t\\tpopulate_tweets_SHA();\\n\\t\\tpopulate_bins_SHA();\\n\\t\\tpopulate_similarity_SHA();\\n\\t}'.replace('SHA', project_name)\n",
    "    controller_parse.insert(ix+1, new_selectedDataset)\n",
    "\n",
    "    # replace existing controller.js\n",
    "    controller_combine = '\\n'.join(controller_parse)\n",
    "    os.remove(os.path.join(path_tf, 'scripts', 'controller.js'))\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'w') as file:\n",
    "        file.write(controller_combine)\n",
    "\n",
    "    print('controller.js modified,     40% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"controller.js modified,     40% complete.\" will be printed out in the terminal. The new *controller.js* should have the following changes being made. We still use the example of the new project called \"Fre\", notice here that the function names (created in function **transform_doc**) will have the project name \"Fre\" at the end:\n",
    "```javascript\n",
    "var idToName = {\n",
    "\t\t\t\t\t// add new idToName\n",
    "\t\t\t\t\t\"Fre\":\"Fre\",\n",
    "\t\t\t\t\t\"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```\n",
    "and\n",
    "```javascript\n",
    "// Populate the interface with the selected data set\n",
    "if (selected_data===\"Full_Disclosure_2012\") {\n",
    "    populate_tweets_Full_Disclosure_2012();\n",
    "    populate_bins_Full_Disclosure_2012();\n",
    "    populate_similarity_Full_Disclosure_2012();\n",
    "}\n",
    "// add new selected dataset here\n",
    "if (selected_data===\"Fre\") {\n",
    "    populate_tweets_Fre();\n",
    "    populate_bins_Fre();\n",
    "    populate_similarity_Fre();\n",
    "}\n",
    "// end of adding new selected datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3 - transform_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we move to the part of the actual data transformation. Functions **transform_doc**, **transform_bins**, and **transform_topicSimilarity** will load the necessary datasets that the user intends to visualize in TopicFlow, transform the data into the format that TopicFlow can read, and create a JavsScript file in the new project data directory.\n",
    "\n",
    "In order to transform data, I think it's worth spending some time doing reverse engineering. Let's first understand what's the end result of **transform_doc** and how it works. The end result is a file called *Doc.js* inside the project data directory. Say the name of the new project is \"Fre\", the path of the end result would be `/topicflow/data/Fre/Doc.js`. *Doc.js* is essentially a JavaScript function that contains all the document text and the metadata of the document, and calling another function defined in *controller.js* to read the data. The skeleton of *Doc.js* looks like:\n",
    "```javascript\n",
    "function populate_tweets_Fre(){\n",
    "    var tweet_data ={\"1\":{\"tweet_id\":1,\"author\":...,\"tweet_date\":...,\"text\":...}, \"2\":...\n",
    "    readTweetJSON(tweet_data);\n",
    "}\n",
    "```\n",
    "If you open it for the first time, the length of this file would be daunting, but it actually has a very simple structure. First, a JavaScript function called **populate_tweets_Fre** (\"Fre\" is the project name) is defined. Then, a variable called \"tweet_data\" is defined, along with literally all the document data in JSON format as the value of this variable. At last, the function **readTweetJSON** defined in *controller.js* is called to actually read the data in \"tweet_data\" variable. \n",
    "\n",
    "One thing important to clarify here is the word \"tweet\", or \"tweets\". Although we are utilizing TopicFlow to read data other than tweets, the file names and function names in TopicFlow inherit the nature of the initial purpose by putting \"tweet\" or \"tweets\" in them. There are so many functions and codes in different files having \"tweet\" and they are so interwined that I couldn't alter this naming convention at this stage. But luckily we can name this file as *Doc.js* instead of *Tweet.js*. Hooray!\n",
    "\n",
    "Okay, now let's see what the JSON part in *Doc.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"1\": {\n",
    "    \"tweet_id\": 1,\n",
    "    \"author\": \"Luciano Bello <luciano () debian org>\",\n",
    "    \"tweet_date\": \"12\\/31\\/2013 16:46\",\n",
    "    \"text\": \"...\"\n",
    "    }\n",
    "  \"2\": {\n",
    "    ...\n",
    "  }\n",
    "  ...\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first store our document data (mainly in .txt files) and metadata in a pandas.DataFrame object (I chose to use pandas.DataFrame here because there are no nesting dictionaries or lists for each document, and this way if we want to see the structure in table we can do that), and transform it into JSON format. Then, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *Doc.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_doc](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/function%20graph/transform_doc.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_doc(project_name, path_doc):\n",
    "    \"\"\"\n",
    "    Transform Full Disclosure email documents in .txt formats into\n",
    "    JavaScript format that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_doc     -- path of documents directory\n",
    "\n",
    "    Returns:\n",
    "        a JavaScript formatted string ready to be written as \"Doc.js\".\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINE month_list, READ DATA\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    df_list = read_data(df_list=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION\n",
    "    # initiate four elements of Doc.js\n",
    "    tweet_id = None\n",
    "    author = []\n",
    "    tweet_date = []\n",
    "    text = []\n",
    "\n",
    "    # populate tweet_id\n",
    "    tweet_count = 0\n",
    "    for month_ix in range(len(month_list)):\n",
    "        tweet_count += len(df_list[month_ix])\n",
    "    tweet_id = list(range(1, tweet_count + 1))\n",
    "\n",
    "    # populate author\n",
    "    for month_ix in range(len(month_list)):\n",
    "        author += df_list[month_ix].author.apply(lambda x: x.replace('\"','')).tolist()\n",
    "\n",
    "    # populate tweet_date\n",
    "    for month_ix in range(len(month_list)):\n",
    "        # transform time into \"mm/dd/yy hh:mm\" format\n",
    "        tweet_date += pd.to_datetime(df_list[month_ix].dateStamp).apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).tolist()\n",
    "\n",
    "    # populate text\n",
    "    for month_ix in range(len(month_list)):\n",
    "        M = df_list[month_ix]\n",
    "        for text_ix in range(len(M)):\n",
    "            # 'k' points to the name of the file\n",
    "            ix = str(M['k'].values[text_ix])\n",
    "            # iterate and read .txt files of a month, add text to a list\n",
    "            # it's worth noting the encoding is 'latin1'\n",
    "            try:\n",
    "                for file in os.listdir(path_doc):\n",
    "                    if file.endswith(\".txt\"):\n",
    "                        yr = file[:4]\n",
    "                        break\n",
    "                filename = yr + '_' + month_list[month_ix] + '_' + ix + '.txt'\n",
    "                path_file = os.path.join(path_doc, filename)\n",
    "                with open(path_file, 'r',\n",
    "                          encoding='latin1') as textfile:\n",
    "                    tmp = textfile.read().replace('\"','').replace('http://','').replace('\\\\','').replace('\\n','')\n",
    "                text.append(tmp)\n",
    "            except:\n",
    "                text.append('empty document')\n",
    "\n",
    "    ### TRANSFORM INTO JS FORMAT\n",
    "    # transform into pd.DataFrame\n",
    "    df_tmp = pd.DataFrame({'tweet_id':tweet_id, 'author':author, 'tweet_date': tweet_date, 'text': text},\n",
    "                          columns=['tweet_id','author','tweet_date','text'],\n",
    "                          index=tweet_id)\n",
    "\n",
    "    # transform body into .json format\n",
    "    json_tmp = df_tmp.to_json(orient='index')\n",
    "\n",
    "    # transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_tweets_' + project_name + '(){\\nvar tweet_data ='\n",
    "    posfix = ';\\nreadTweetJSON(tweet_data);\\n}'\n",
    "    doc_js = prefix + json_tmp + posfix\n",
    "\n",
    "\n",
    "    ### WRITE\n",
    "    # make a directory named after project_name\n",
    "    if os.path.isdir(os.path.join(path_tf, 'data', project_name)) == False:\n",
    "        os.mkdir(os.path.join(path_tf, 'data', project_name))\n",
    "\n",
    "    # write\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'Doc.js'), 'w') as file:\n",
    "        file.write(doc_js)\n",
    "\n",
    "    print('Doc.js created,             60% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"Doc.js created,             60% complete.\" will be printed out in the terminal. This newly created file should populate the document content on the right side of TopicFlow. Clicking a document should let a uer see the author, date, and actual text of that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4 - transform_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_bins** is the hardest part in the whole data transformation pipeline. Although it has the same three-part-structure as **transform_doc**, the JSON part in **transform_bins** is much more complex, thus require very careful handling of indexing and putting data in the right place. Here we can take a quick glance of the model that's draw by Carlos Paradis:\n",
    "![bins_model](https://raw.githubusercontent.com/estepona/topicflow/master/data_model/bins_model.png)\n",
    "\n",
    "Again, let's do reverse engineering. The end result is a file called *Bins.js* inside the project data directory. Say the name of the new project is \"Fre\", the path of the end result would be `/topicflow/data/Fre/Bins.js`. *Bins.js* is essentially a JavaScript function that divide all documents by time (in the example of Full Disclosure data, divide by month) which is called binning, and store the LDA data (document-topic scores and topic-word scores) of all the pairs. The skeleton of *Bins.js* looks like:\n",
    "```javascript\n",
    "function populate_bins_Fre(){\n",
    "    var bin_data ={\"0\":{\"tweet_Ids\":[...],\"start_time\":...,\"bin_id\":...,\"topic_model\":{...},\"end_time\":...},\"1\":...\n",
    "    readBinJSON(bin_data);\n",
    "}\n",
    "```\n",
    "Again, it's could be daunting the first time you open it: it's very lengthy, but the structure stays the same. First, a JavaScript function called **populate_bins_Fre** (\"Fre\" is the project name) is defined. Then, a variable called \"bin_data\" is defined, along with all the relevent data in JSON format as the value of this variable. At last, the function **readBinJSON** defined in *controller.js* is called to read the data in \"bin_data\" variable. \n",
    "\n",
    "Now let's see what the JSON part in *Bins.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"0\": {\n",
    "    \"tweet_Ids\": [1,2,3...],\n",
    "    \"start_time\": \"12/31/2013 16:46\",\n",
    "    \"bin_id\": 0,\n",
    "    \"topic_model\": {\n",
    "            \"topic_doc\": {\n",
    "                    \"0_0\": {\n",
    "                        \"1\": 0.00010030434072387,\n",
    "                        \"2\": 0.36551017173243494,\n",
    "                        ...\n",
    "                    },\n",
    "                    \"0_1: {...},\n",
    "                    ...\n",
    "                },\n",
    "            \"doc_topic\": {\n",
    "                \"1\": {\n",
    "                    \"0_0\": 0.00010030434072387,\n",
    "                    \"0_1\": 0.00010030434072383,\n",
    "                    ...\n",
    "                },\n",
    "                \"2\": {...},\n",
    "                ...\n",
    "            },\n",
    "            \"topic_word\": {\n",
    "                \"0_0\": {\n",
    "                    \"x86_64\": 0.0361921097895964,\n",
    "                    \"i586\": 0.0335562698609424,\n",
    "                    ...\n",
    "                },\n",
    "                \"0_1\": {...},\n",
    "                ...\n",
    "            },\n",
    "            \"topic_prob\": {\n",
    "                \"0\": \"0_0\",\n",
    "                \"1\": \"0_1\",\n",
    "                ...\n",
    "            }\n",
    "        },\n",
    "    \"end_time\": \"1/31/2014 21:25\"\n",
    "    }\n",
    "  \"1\": {\n",
    "    ...\n",
    "  }\n",
    "  ...\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first process and store all the data in a dictionary, and transform it into JSON format. Then, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *Bins.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_bins](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/function%20graph/transform_bins.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of how **transform_bins** works can be found in the comments. One thing to notice is that this function takes more consideration in indexing than other functions because there are so many document-topic and topic-word pairs to populate and sometimes the index starts with 0 and sometimes it starts with 1, a consistancy issue that's hard to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_bins(project_name, path_doc, path_LDA):\n",
    "    \"\"\"\n",
    "    Transform LDA-genereted Topic-document matrixes and Topic-Term\n",
    "    matrixes into JavaScript format that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_doc     -- path of documents directory\n",
    "        path_LDA     -- path of LDA main directory, this directory should\n",
    "                        contain 3 sub-directories: Document_Topic_Matrix,\n",
    "                        Topic_Flow, and Topic_Term_Matrix\n",
    "\n",
    "    Returns:\n",
    "        a JavaScript formatted string ready to be written as \"Bins.js\".\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINE month_list, READ DATA\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    # read df_list\n",
    "    df_list = read_data(df_list=True)\n",
    "    # read topic-doc & topic-word data sets\n",
    "    df_topic_doc = read_data(df_topic_doc=True)\n",
    "    # read topic-word data sets\n",
    "    df_topic_word = read_data(df_topic_word=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION - 1\n",
    "    # initiate bins, each month is one bin, each bin is also a dictionary\n",
    "    bin_dict = {}\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)] = {}\n",
    "\n",
    "    # populate bin_id\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['bin_id'] = month_ix\n",
    "\n",
    "    # populate tweet_ids\n",
    "    # here we need input from df_list, specifically the lenth of each month\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['tweet_Ids'] = []\n",
    "    # two points recording the starting position of tweet_id of each month\n",
    "    lo,hi = 1,1\n",
    "    for month_ix in range(len(month_list)):\n",
    "        hi += len(df_list[month_ix])\n",
    "        for tweet_ix in range(lo,hi):\n",
    "            bin_dict[str(month_ix)]['tweet_Ids'].append(tweet_ix)\n",
    "        lo = hi\n",
    "\n",
    "    # populate start_time & end_time\n",
    "    # here we need input from df_list, specifically the lenth of each month\n",
    "    # this part sorts out the earliest and latest time of a tweet in each month, and\n",
    "    # transform them into \"mm/dd/yy hh:mm\" format\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['start_time'] = pd.to_datetime(df_list[month_ix].dateStamp).sort_values().apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).tolist()[0]\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['end_time'] = pd.to_datetime(df_list[month_ix].dateStamp).sort_values().apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).tolist()[-1]\n",
    "\n",
    "\n",
    "    # initiate topic_model\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['topic_model'] = {}\n",
    "        # add 4 sub dictionaries\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_doc'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['doc_topic'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_word'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_prob'] = {}\n",
    "\n",
    "\n",
    "    ###  DATA TRANSFORMATION - 2: POPULATE topic_model\n",
    "\n",
    "    # to begin this section, create a DataFrame mapping Topic-doc.\n",
    "    # the documents in the df_topic_doc are not the same as in metadata.\n",
    "    # Thus, before pupulating 4 sub dictionaries, first we need to find\n",
    "    # all the overlapping documents\n",
    "\n",
    "    # step 1, creates a list of the starting position of each month's tweet_id\n",
    "    month_start_tweetIds = []\n",
    "    tweet_count = 0\n",
    "    for month_ix in range(len(month_list)):\n",
    "        month_start_tweetIds.append(tweet_count)\n",
    "        tweet_count += len(df_list[month_ix])\n",
    "\n",
    "    # step 2, iterate and find the overlapping documents of every month\n",
    "    for month_ix in range(len(month_list)):\n",
    "        doc_df_topic_doc = []\n",
    "        for i in df_topic_doc[month_ix].index.values:\n",
    "            doc_df_topic_doc.append(int(i[13:-4]))\n",
    "        overlap = set(doc_df_topic_doc) & set(df_list[month_ix]['k'].values)\n",
    "\n",
    "        # step 3, create a DataFrame mapping the overlapping documents and 10 topics\n",
    "        overlap_ix = []\n",
    "        ix_list = df_topic_doc[month_ix].index.tolist()\n",
    "        doc_year = df_topic_doc[0].index.values[0][4:8]\n",
    "        for item in overlap:\n",
    "            name = str(month_list[month_ix]) + '/' + doc_year + '_' + str(month_list[month_ix]) + '_' + str(item) + '.txt'\n",
    "            overlap_ix.append(ix_list.index(name))\n",
    "        df_topic_doc_overlap = df_topic_doc[month_ix].iloc[overlap_ix, : ].copy()\n",
    "\n",
    "        # pre-step 4, add tweet_Ids to df_topic_doc_overlap\n",
    "        overlap_tweetIds = []\n",
    "        for k in df_topic_doc_overlap.index.values:\n",
    "            name = int(k[13:-4])\n",
    "            name_ix = df_list[month_ix]['k'].tolist().index(name) + 1\n",
    "            name_ix += month_start_tweetIds[month_ix]\n",
    "            overlap_tweetIds.append(name_ix)\n",
    "        df_topic_doc_overlap['tweet_Ids'] = overlap_tweetIds\n",
    "\n",
    "        # now we have the overlapping documents, we can populate 4 sub dictionaries\n",
    "        # populate topic_prob\n",
    "        L = len(df_topic_doc[month_ix].columns)\n",
    "        for ix in range(L):\n",
    "            T = str(month_ix) + '_' + str(ix)\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_prob'][str(ix)] = T\n",
    "\n",
    "        # populate topic_doc\n",
    "        # create 10 topic keys\n",
    "        for ix in range(L):\n",
    "            T = str(month_ix) + '_' + str(ix)\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_doc'][T] = {}\n",
    "        # add doc values to these keys\n",
    "        for ix_2 in range(L):\n",
    "            T = str(month_ix) + '_' + str(ix_2)\n",
    "            col_score = df_topic_doc_overlap[str(ix_2 + 1)].values # there is +1 here because in the csv there is no column named '0'\n",
    "            col_score = np.around(col_score, 17)                 # reduce crazy long decimal points and scientific notations\n",
    "            col_k = df_topic_doc_overlap['tweet_Ids'].values\n",
    "            for ix_3 in range(len(col_score)):\n",
    "                bin_dict[str(month_ix)]['topic_model']['topic_doc'][T][str(col_k[ix_3])] = col_score[ix_3]\n",
    "\n",
    "        # populate doc_topic\n",
    "        for ix_4 in range(len(df_topic_doc_overlap)):\n",
    "            row_score = df_topic_doc_overlap.iloc[ix_4,:]\n",
    "            row_score = np.around(row_score, 17)\n",
    "            bin_dict[str(month_ix)]['topic_model']['doc_topic'][ str(int(row_score['tweet_Ids'])) ] = {}\n",
    "            for ix_5 in range(L):\n",
    "                name = str(month_ix) + '_' + str(ix_5)\n",
    "                bin_dict[str(month_ix)]['topic_model']['doc_topic'][ str(int(row_score['tweet_Ids'])) ][name] = row_score[ix_5]\n",
    "\n",
    "        # populate topic_word\n",
    "        for ix_6 in range(L):\n",
    "            name = str(month_ix) + '_' + str(ix_6)\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_word'][name] = {}\n",
    "            topwords = df_topic_word[month_ix].iloc[ix_6].sort_values(ascending=False)[:10]\n",
    "            topwords = np.around(topwords, 17)\n",
    "            # we choose top 10 most frequent words, so here the range is 10\n",
    "            for ix_7 in range(10):\n",
    "                bin_dict[str(month_ix)]['topic_model']['topic_word'][name][topwords.index[ix_7]] = topwords.values[ix_7]\n",
    "\n",
    "        # delete df_topic_doc_overlap to aviod overwritting error\n",
    "        del df_topic_doc_overlap\n",
    "\n",
    "    ### TRANSFORM INTO JS FORMAT\n",
    "    # transform bin_dict into an ordered dictionary\n",
    "    bin_dict_ordered = {}\n",
    "\n",
    "    key_order = ('tweet_Ids','start_time','bin_id','topic_model','end_time')\n",
    "    for month_ix in range(len(month_list)):\n",
    "        tmp = OrderedDict()\n",
    "        for k in key_order:\n",
    "            tmp[k] = bin_dict[str(month_ix)][k]\n",
    "        bin_dict_ordered[str(month_ix)] = tmp\n",
    "\n",
    "    # transform body into .json format\n",
    "    json_tmp = json.dumps(bin_dict_ordered)\n",
    "\n",
    "    # transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_bins_' + project_name + '(){\\nvar bin_data = '\n",
    "    posfix = ';\\nreadBinJSON(bin_data);\\n}'\n",
    "    bins_js = prefix + json_tmp + posfix\n",
    "\n",
    "\n",
    "    ### WRITE\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'Bins.js'), 'w') as file:\n",
    "        file.write(bins_js)\n",
    "\n",
    "    print('Bins.js created,            80% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"Bins.js created,            80% complete.\" will be printed out in the terminal. This newly created file should populate the both the bottom-left and center area of TopicFlow. Each column in the visualization is a bin and each box is a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5 - transform_topicSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After bins and topics are created, **transform_topicSimilarity** generates nodes and links between topics in adjacent bins. It also has the three-part-structure as **transform_doc** and **transform_bins**. \n",
    "\n",
    "Reverse engineering! The end result is a file called *TopicSimilarity.js* inside the project data directory. Say the name of the new project is \"Fre\", the path of the end result would be `/topicflow/data/Fre/TopicSimilarity.js`. *TopicSimilarity.js* is essentially a JavaScript function that scores how similar the topics between two adjancent bins are. The scores are also generated by the LDA algorithm. The skeleton of *Bins.js* looks like:\n",
    "```javascript\n",
    "function populate_similarity_Fre(){\n",
    "    var sim_data ={\"nodes\":[{\"name\":...,\"value\":...},...],\"links\":[{\"source\":...,\"target\":...,\"value\":...},...]}\n",
    "    readSimilarityJSON(sim_data);\n",
    "}\n",
    "```\n",
    "*TopicSimilarity.js* is the shortest among all three data files and it follows a simple logic: we have nodes and score the links between nodes. As of the overall JavaScript structure, first, a function called **populate_similarity_Fre** (\"Fre\" is the project name) is defined. Then, a variable called \"sim_data\" is defined, along with all the relevent data in JSON format as the value of this variable. At last, the function **readSimilarityJSON** defined in *controller.js* is called to read the data in \"sim_data\" variable. \n",
    "\n",
    "Now let's see what the JSON part in *TopicSimilarity.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"nodes\": [\n",
    "      {\n",
    "          \"name\": \"0_0\",\n",
    "          \"value\": 43\n",
    "      },\n",
    "      {\n",
    "          \"name\": \"0_1\",\n",
    "          \"value\": 57\n",
    "      },\n",
    "      ...\n",
    "  ],\n",
    "  \"links\": [\n",
    "      {\n",
    "          \"source\":1,\n",
    "          \"target\":18,\n",
    "          \"value\":233.6647080989732\n",
    "      },\n",
    "      {\n",
    "          \"source\":2,\n",
    "          \"target\":13,\n",
    "          \"value\":183.70069470814772\n",
    "      },\n",
    "      ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first process and store all the similarity data in a dictionary, and transform it into JSON format. Then, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *TopicSimilarity.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_topicSimilarity](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/function%20graph/transform_topicSimilarity.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_topicSimilarity(project_name, path_LDA):\n",
    "    \"\"\"\n",
    "    Transform topic similarity matrix into JavaScript format\n",
    "    that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_LDA     -- path of LDA main directory, this directory should\n",
    "                        contain 3 sub-directories: Document_Topic_Matrix,\n",
    "                        Topic_Flow, and Topic_Term_Matrix\n",
    "\n",
    "    Returns:\n",
    "        a JavaScript formatted string ready to be written as\n",
    "        \"TopicSimilarity.js\".\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINE month_list, READ DATA\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    df_topic_sim = read_data(df_topic_sim=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION\n",
    "    # initiate a dictionary\n",
    "    sim_dict = {}\n",
    "\n",
    "    # populate nodes\n",
    "    # put topics into nodes, record their orders\n",
    "    nodes = []\n",
    "    for i in range(len(month_list)):\n",
    "        for j in range(10):\n",
    "            tmp = {}\n",
    "            name = str(i) + '_' + str(j)\n",
    "            # how to calculate the value of a topic? the paper didn't define clearly\n",
    "            # so here I use a random number\n",
    "            value = np.random.randint(1,100)\n",
    "            tmp['name'], tmp['value'] = name, value\n",
    "            nodes.append(tmp)\n",
    "\n",
    "    # populate links\n",
    "    # put source, target, value into links\n",
    "    links = []\n",
    "    for month_ix in range(len(month_list) - 1):\n",
    "        # get unique pais between every two months, in total we have 11 pairs\n",
    "        mm1, mm2 = month_list[month_ix], month_list[month_ix + 1]\n",
    "        sim = mm1 + '_' + mm2 + '_similarity'\n",
    "        df_tmp = df_topic_sim[[mm1, mm2, sim]].dropna(axis=0).drop_duplicates()\n",
    "        for row_ix in range(len(df_tmp)):\n",
    "            source = month_ix*10 + int(df_tmp[mm1].values[row_ix]) - 1\n",
    "            target = (month_ix+1)*10 + int(df_tmp[mm2].values[row_ix]) - 1\n",
    "            score = df_tmp[sim].values[row_ix] * 200 # 200 makes it neither too thin nor too thick\n",
    "            link_tmp = {}\n",
    "            link_tmp['source'], link_tmp['target'], link_tmp['value'] = source, target, score\n",
    "            links.append(link_tmp)\n",
    "\n",
    "    # put two lists into sim_dict\n",
    "    sim_dict['nodes'], sim_dict['links'] = nodes, links\n",
    "\n",
    "\n",
    "    ### TRANSFORM INTO JS FORMAT\n",
    "    json_tmp = json.dumps(sim_dict)\n",
    "\n",
    "    # finally, transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_similarity_' + project_name + '(){\\nvar sim_data = '\n",
    "    posfix = ';\\nreadSimilarityJSON(sim_data);\\n}'\n",
    "    topicSimilarity_js = prefix + json_tmp + posfix\n",
    "\n",
    "\n",
    "    ### WRITE\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'TopicSimilarity.js'), 'w') as file:\n",
    "        file.write(topicSimilarity_js)\n",
    "\n",
    "    print('TopicSimilarity.js created, 100% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"TopicSimilarity.js created, 100% complete.\" will be printed out in the terminal. This newly created file should control the top-left panel of TopicFlow and the lines between different topics. These data are in charge of topic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 6 - argparse and local server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, under   \n",
    ">`if __name__ == \"__main__\":`  \n",
    "\n",
    "two functionalities are added to allow malnipulation in terminal and local server instance. \n",
    "\n",
    "Using the argparse library in `run.py` makes it easier for a user to add a project in terminal and see the TopicFlow visualization in a local server, or run an existing project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ARGPARSE\n",
    "parser = argparse.ArgumentParser(prog = 'TopicFlow Creator',\n",
    "                                 description = 'A program that lets you create a new project and transforms your data into TopicFlow readable format, or run an existing project.',\n",
    "                                 epilog = 'Then you can open a browser and type in localhost:8000 to see the visualization! When done, just stop the process in terminal.')\n",
    "parser.add_argument('-n', '--new',  type = str,\n",
    "                    help = 'Enter the name of a new project, no space allowed.')\n",
    "parser.add_argument('-a', '--add', type = str, nargs = '+',\n",
    "                    help = 'Please specify the paths of [document files, LDA files], enclosing each in double quotes. If starting a new project, both paths should be specified. If running an existing project, no need to use this flag. EXAMPLE: -n \"Trending\" -a \"E:\\\\...\\\\data\\\\docs\" \"E:\\\\...\\\\data\\\\LDA\".')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "if args.new:\n",
    "    project_name = args.new\n",
    "    path_doc = args.add[0]\n",
    "    path_LDA = args.add[1]\n",
    "\n",
    "    if os.path.isdir(path_doc) and os.path.isdir(path_LDA):\n",
    "        modify_html(project_name, path_tf)\n",
    "        modify_controller(project_name, path_tf)\n",
    "        transform_doc(project_name, path_doc)\n",
    "        transform_bins(project_name, path_doc, path_LDA)\n",
    "        transform_topicSimilarity(project_name, path_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new project using Full Disclosure 2012 document and LDA data in terminal looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run.py -n -a](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/screenshots/run%20-n%20-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**command**:\n",
    "\n",
    "`python topicflow\\run.py -n \"Fre\" -a \"E:\\documents\\Learning Materials\\from_UMD\\projects\\PERCEIVE\\data\\Full Disclosu re\\2012 - Copy\" \"E:\\documents\\Learning Materials\\from_UMD\\projects\\PERCEIVE\\data\\LDA_VEM\\2012_k_10_12\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A timer is also included to see how long the data transformation takes. Just nice to know.\n",
    "\n",
    "Now, let us see the end result of our data transformation in local:8000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INVOKE SERVER\n",
    "PORT = 8000\n",
    "\n",
    "# change the working directory to topicflow\n",
    "os.chdir(path_tf)\n",
    "\n",
    "Handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n",
    "    print(\"serving at port\", PORT)\n",
    "    httpd.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TopicFlow Fre](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/screenshots/TopicFlow-Fre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with the visualization, just stop the process in terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although now we have a working data transformation pipeline, there are still some issues remained:\n",
    "\n",
    "    \n",
    "1. **Search in TopicFlow**  \n",
    "    Search is a useful functionality in the original version that allows a user searching key words in bottom-left panel. Based on user's search, irrelevent topics, nodes, links, and documents will be filtered out. However, because we disabled \n",
    "    ```javascript\n",
    "    if ($(\"g #\"+j)[0].style.display != \"none\") { }\n",
    "    ```\n",
    "    at the beginning in order to avoid `style errors`, we also made TopicFlow unable to show relevent topics and documents after user's search (nodes and links will be shown). If we enable this line, for some reason TopicFlow wouldn't load Full Disclosure documents. Working around this line of code took me quite some time, and yet I don't know how I could keep this code while making TopicFlow workable.\n",
    " \n",
    "2. **Value of node**  \n",
    "    In function **transform_topicSimilarity**, the value of each individual node is not clearly defined in the original paper, so the way I approach this is generating a random integer between 1 and 100 and assign it to the value of node.\n",
    "    ```python\n",
    "    # how to calculate the value of a topic? the paper didn't define clearly\n",
    "    # so here I use a random number\n",
    "    value = np.random.randint(1,100)\n",
    "    tmp['name'], tmp['value'] = name, value\n",
    "    ```\n",
    "    I'd like to know how the values are defined and make changes to the data transformation pipeline accordingly.\n",
    "\n",
    "3. **Additional options to manage projects**  \n",
    "    Currently the `run.py` only allows creating a new project or running an existing project. In the future we may want to add additional options such as removing a project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
